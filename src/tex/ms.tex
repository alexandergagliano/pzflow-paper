% Define document class
\documentclass[twocolumn]{aastex631}
\usepackage{showyourwork}

\usepackage{tikz}
\usetikzlibrary{positioning}
\usetikzlibrary{shapes}
\usetikzlibrary{fit}

\newcommand{\comment}[1]{}

% note command for inline notes
\newcommand{\note}[1]{\textsf{\textcolor{red}{#1}}}
% command to note a missing citation
\newcommand{\needscite}{\note{[needs citation]}}

% some commonly used math expressions
\newcommand{\px}{p^{}_{X}}
\newcommand{\pu}{p^{}_{U}}
\newcommand{\R}{\mathbb{R}}

\shorttitle{Modeling Galaxy Catalogues with Normalizing Flows}
\shortauthors{Crenshaw et al.}

% Begin!
\begin{document}

% Title
\title{Statistical Modeling of Galaxy Catalogues with Normalizing Flows}

% Author list
\correspondingauthor{John Franklin Crenshaw}
\email{jfc20@uw.edu}

\author[0000-0002-2495-3514]{John Franklin Crenshaw}
\affiliation{DIRAC Institute and Department of Physics, University of Washington, Seattle, WA 98195, USA}

\author{J. Bryce Kalmbach}

\author{Alexander Gagliano}

\author{Ziang Yan}

\author{Andrew Connolly}

\author{Francois Lanusse}

\author{Alex Malz}

\author{Sam Schmidt}

\collaboration{99}{(The LSST Dark Energy Science Collaboration)}


\begin{abstract}
    Normalizing flows are powerful tools for learning high-dimensional probability distributions from samples thereof, and have many applications in astronomy, including posterior estimation and forward modeling.
    We introduce PZFlow, a Python package for the statistical modeling of tabular data using normalizing flows.
    We use PZFlow to model photometric galaxy catalogs including redshifts, LSST photometry, size, and ellipticity.
    We generate synthetic galaxy catalogs in which each galaxy has a ``true'' redshift posterior distribution, and discuss the importance of these true posteriors for the comprehensive evaluation of the redshift posteriors produced by photometric redshift (photo-z) estimators.
    We also demonstrate the use of an ensemble of normalizing flows for photo-z estimation.
    While we focus on photo-z estimation and validation, we emphasize that these methods are applicable to any galaxy properties, and any other tabular data.
\end{abstract}


\section{Introduction}
\label{sec:intro}

Astronomical data represent realizations of complex probability distributions.
A common goal in research is to infer the underlying distribution from a limited set of noisy data, and to use this distribution to estimate posterior distributions over galaxy and population parameters.
An important example is photometric redshift (photo-z) estimation, where galaxy redshift posteriors are estimated from galaxy photometry, using a model informed by a training set of spectroscopic galaxy data \citep{newman2022}.
These redshift posteriors are then used in analyses that estimate posterior distributions for the cosmological parameters of interest \citep{descSRD}.

Knowledge of the underlying probability distributions is also valuable for simulation and forward modeling astronomical data sets.
Simulating data sets with realistic statistical properties enables methodological development and the calibration of systematic uncertainties.
Forward modeling also facilitates data augmentation (e.g. \citealt{lokken2022}) and the creation of large data challenges (e.g. \citealt{kessler2019, dc2, cosmodc2}), which are becoming more prevalent in the big data era of astronomy.

Inferring the underlying probability distribution from a high-dimensional data set is a difficult problem, and many machine learning tools have been developed for this task.
Two popular examples from deep learning are Generative Adversarial Networks (GANs; \citealt{goodfellow2014}) and Variational Autoencoders (VAEs; \citealt{kingma2014}).
Both methods excel at forward modeling complex data sets, but neither allow exact likelihood inference.

Normalizing flows are a deep learning tool that excel at forward modeling, but also allow exact likelihood inference.
They operate by learning an invertible transformation of the data distribution into a simpler, tractable distribution (such as a normal distribution, hence the name).
This allows sampling and likelihood inference to be performed within the context of the simple distribution, with the normalizing flow acting as a translator between these samples and likelihoods, and their corresponding values in the context of the data distribution.

Once trained, a normalizing flow can forward model data with realistic statistical correlations, and since normalizing flows provide exact likelihood inference, each property of the generated data has a ``true'' posterior.
When calculating these posteriors, you can convolve error distributions, and marginalize over chosen properties.
Access to ``true'' posteriors is very valuable for the development and validation of analysis pipelines that include posterior estimation, such as the photo-z estimators used in much of astrophysics and cosmology.

To facilitate the statistical modeling of galaxy catalogs and other astronomical data sets, we have developed PZFlow, a normalizing flow package for python.
With relatively little tuning required by the user, PZFlow can provide a generative model for any tabular data, including continuous and discrete variables, and variables with Euclidean or periodic topology.
In addition to generative modeling, PZFlow can calculate posteriors over any columns in your data set, and can convolve errors and marginalize over missing columns while training the model or calculating posteriors for samples.

In this paper, we provide the background on normalizing flows (Section \ref{sec:nf}) required to understand PZFlow (Section \ref{sec:pzflow}).
We then use PZFlow to simulate a galaxy catalog (Section \ref{sec:galaxy-catalog}), where each object has photometry (including errors), size, ellipticity, redshift, and a true redshift posterior.
We also demonstrate using PZFlow as a density estimator, via the example of photo-z estimation (Section \ref{sec:photo-z}).
We conclude in Section \ref{sec:conclusion}.


\section{Normalizing Flows}
\label{sec:nf}

\begin{figure*}[t!]
    \script{intro/plot_two_moons.py}
    \begin{centering}
        \includegraphics{figures/twomoons_example.pdf}
        \caption{
            A normalizing flow demonstrated on the two moons data set from scikit-learn.
            The two moons data on the left is mapped onto a two dimensional uniform distribution by the bijection $f$.
            The data are colored by quadrant to visualize their image in the latent space.
            You can sample the data distribution by sampling from the uniform distribution, and using $f^{-1}$ to map the samples back to the data space.
        }
        \label{fig:two-moons}
    \end{centering}
\end{figure*}

Normalizing flows model complex, high-dimensional probability distributions by learning a mapping from the data distribution to a tractable latent distribution\footnote{Some of the machine learning literature defines the mapping in the opposite direction.}.
Often the latent distribution is a standard Normal distribution, and so the mapping ``normalizes'' the data, hence the name ``noramlizing flow''.
This mapping allows us to sample and evaluate densities using the latent distribution, rather than the unknown data distribution.

Assume we have a differentiable function $f$ that maps samples $x$ from the data distribution $\px$ onto samples $u$ from the latent distribution $\pu$.
Using the change of variables formula, we can evaluate the probability density of the data:
\begin{align}
    \px(x) = \pu(u=f(x)) \, |\det \nabla f(x)|,
    \label{eq:px}
\end{align}
where $\nabla f(x)$ is the Jacobian of $f$ evaluated at $x$.
In words, computing the density $\px(x)$ is accomplished by mapping $x$ to the latent distribution, calculating its density there, and multiplying by the associated Jacobian determinant, which accounts for how the function $f$ distorts volume elements of the space.

If we further assume that is invertible, we can sample from the data distribution by appling $f^{-1}$ to samples from the latent distribution:
\begin{align}
    x = f^{-1}(u) \quad \text{where} \quad u \sim \pu.
\end{align}

Figure \ref{fig:two-moons} shows an example of a normalizing flow that transforms the scikit-learn \citep{sklearn} two moons distribution into a uniform distribution.
The data points are colored by quadrant to visualize their image under $f$.

The following sections discuss how to build a normalizing flow to model data with various features.
Section \ref{sec:bijections} discusses the bijection $f$ and introduces the building blocks from which our bijections will be built;
Section \ref{sec:latent} discusses how to choose an appropriate latent distribution for your data;
Section \ref{sec:conditional} describe how to condition a normalizing flow on some conditional variables;
Section \ref{sec:periodic} explains how to model data with periodic topology;
finally Section \ref{sec:discrete} explains how to model data with discrete variables.


\subsection{Designing a bijection}
\label{sec:bijections}

The bijection of a normalizing flow must be powerful enough to model complex relationships in data, while simultaneously possessing an efficiently computable Jacobian determinant.
This latter constraint is the primary difficulty in designing a normalizing flow.
The most popular strategy for achieving these requirements is to exploit the fact that a composition of bijections is also bijective.
By chaining together multiple less-expressive bijections whose Jacobians are efficiently computable, a composite bijections can be constructed that meets our requirements:
\begin{align}
    f &= \dots \circ f_3 \circ f_2 \circ f_1.
\end{align}
The overall Jacobian determinant can be efficiently calculated using the chain rule.

There is an extensive literature on constructing these sub-bijections (see \citealt{kobyzev2020} for a review).
Some bijections are specialized to be particularly efficient at either density estimation or sampling, but for many science cases, we wish to do both.
For this reason, we will focus on Rational-Quadratic Rolling Spline Couplings (RQ-RSCs), which achieve state-of-the-art performance, while being efficient with both tasks \citep{durkan2019}.

\subsubsection{Rational-Quadratic Rolling Spline Couplings}
\label{sec:rq-rsc}

\begin{figure}
    \centering
    \begin{tikzpicture}[thick, outer sep=0]
        \node [draw, rectangle, minimum height=2cm, minimum width=1.2cm, fill=blue!20] (1) {$x^{}_{1:d}$};
        \node [draw, rectangle, minimum height=2cm, minimum width=1.2cm, fill=red!20, below = 0 of 1] (2) {$x^{}_{d+1:D}$};
        \node [draw, circle, minimum size=8mm, fill=gray!20, right = 2cm of 1] (3) {$=$};
        \node [draw, circle, minimum size=8mm, fill=gray!20, right = 2cm of 2] (4) {$g$};
        \node [draw, rectangle, minimum height=2cm, minimum width=1.2cm, fill=blue!20, right = 1.5cm of 3] (5) {$y^{}_{1:d}$};
        \node [draw, rectangle, minimum height=2cm, minimum width=1.2cm, fill=red!20, below = 0 of 5] (6) {$y^{}_{d+1:D}$};
        \draw[->] (1) -- (3);
        \draw[->] (2) -- (4);
        \draw[->] (3) -- (5);
        \draw[->] (4) -- (6);
        \draw[->] (1) -- (4) node [midway, draw, circle, fill=white, minimum size=8mm, fill=gray!20] {$m$};
    \end{tikzpicture}
    \caption{
    Diagram of a coupling layer.
    The first partition, $x^{}_{1:d}$, is passed through the layer unchanged.
    The second partition, $x^{}_{d+1:D}$, is transformed by the coupling law $g$, which is parameterized by the coupling function $m$ applied to the first partition.
    }
    \label{fig:coupling}
\end{figure}

RQ-RSCs are based on coupling layers \citep{dinh2015, dinh2017}.
A coupling layer partitions the data, $x \in \R^D$, into two sets, $x_{1:d}$ and $x_{d+1:D}$.
The first set is then used to transform the second set:
\begin{align}
    \begin{split}
    y^{}_{1:d} &= x^{}_{1:d} \\
    y^{}_{d+1:D} &= g(x^{}_{d+1:D}; m(x^{}_{1:d})),
    \end{split}
\end{align}
where $g : \R^{D-d} \times \R^d \to \R^{D-d}$ is an invertible \emph{coupling law}, and $m$ is a \emph{coupling function} defined on $\R^d$.
This is illustrated in Figure \ref{fig:coupling}.
The advantage of this structure is that the Jacobian is triangular,
\begin{align}
    \frac{\partial y}{\partial x} =
    \begin{pmatrix}
         I_d & 0 \\
         \frac{\partial y^{}_{d+1:D}}{\partial x^{}_{1:d}}
         & \frac{\partial y^{}_{d+1:D}}{\partial x^{}_{d+1:D}}
    \end{pmatrix},
\end{align}
and in particular, the Jacobian determinant is
\begin{align}
    \det \frac{\partial y}{\partial x} = \det \frac{\partial y^{}_{d+1:D}}{\partial x^{}_{d+1:D}}.
\end{align}
Furthermore, the inverse can be calculated as
\begin{align}
    \begin{split}
    x^{}_{1:d} &= y^{}_{1:d} \\
    x^{}_{d+1:D} &= g^{-1}(y^{}_{d+1:D}; m(x^{}_{1:d})),
    \end{split}
\end{align}
Notice that neither inverting a coupling layer nor calculating the Jacobian determinant requires inverting or taking derivatives of the coupling function $m$, which can thus be arbitrarily complex.

The obvious limitation of a coupling later is that only a subset of the data variables are transformed.
This is overcome by stacking multiple coupling layers in succession, and switching which variables belong to which partition.
In practice, this is achieved by interspersing coupling layers with bijections that shuffle the dimensions of $x$.
These shuffling bijections are trivially inverted and have a Jacobian determinant of one.

There are a variety of different coupling laws one can use.
One particularly expressive choice is a Rational-Quadratic Neural Spline Coupling (RQ-NSC; \citealt{durkan2019}).
As the name suggests, the \emph{coupling law} for RQ-NSCs is a set of rational-quadratic splines and the \emph{coupling function} is a neural network.
RQ-NSCs define a spline $g_i: [-B, B] \subset \R \to [-B, B]$ for each dimension $i$ of $x_{d+1:D}$, where $g_i$ is a piecewise combination of $K$ segments, and each segment is a rational-quadratic function.
The positions and derivatives of the knots that parameterize the splines are calculated from $x_{1:d}$ by a neural network.

RQ-NSCs are state-of-the-art for efficient sampling and density estimation \citep{kobyzev2020}, and are flexible enough to model complex distributions with multiple discontinuities and hundreds of modes.
In addition, they are easily adaptable for flows with periodic topology (Section \ref{sec:periodic}).
For more details, see \citet{durkan2019}.

In this work, we introduce a simple extension of RQ-NSCs, which we name Rational-Quadratic Rolling Spline Couplings (RQ-RSCs).
An RQ-RSC models $D$ dimensional data by stacking $D$ RQ-NSC layers (with $d=D-1$), each followed by a Rolling layer.
A Rolling layer shifts the dimensions of $x$ by one place:
\begin{align}
    \mathrm{Roll}: [x_1, \dots , x_{D-1}, x_D] \to [x_D, x_1, \dots , x_{D-1}].
\end{align}
In other words, RQ-RSCs individually transform each of the $D$ dimensions of $x$ as a function of the other $D-1$ dimensions.
In the limit of high spline resolution (i.e. $K \to \infty$), RQ-RSCs can model any differentiable, monotonic function on $[-B, B]^D$ and can thus model arbitrarily complex distributions in this region.
In practice, we find very good performance for diverse data sets with much smaller $K$.

Note that you can also specify a different value of $K$ for each of the $D$ spline layers in order to individually control the resolution of each dimension.

\subsubsection{Data processing bijections}
\label{sec:data-processing}

While RQ-RSCs perform the heavy lifting of mapping the data distribution $\px$ onto the latent distribution $\pu$, it is also convenient to define other bijections that perform useful operations such as pre- and post-processing.
We name these \emph{data processing bijections}.

For example, RQ-RSCs (and the RQ-NSCs on which they are based) are defined on the domain [-B, B], and thus will not transform samples outside this range.
It is therefore useful to define a \emph{Shift Bounds} bijection, which shifts the original range of each dimension to match the domain of the splines.
Note that this shift must be set at training time, with the assumption that future test data will lie within the same bounds\footnote{This bijection may be poorly suited to data with large outliers. In that case, more care should be taken in designing a bijection that maps the data onto the range of the splines.}.
You can choose a range wider than that covered by the training set if you wish to allow the flow to sample outside the range of the training set

For an example of building an application-specific data processing bijection, see the \emph{Color Transform} bijection defined in Section \ref{sec:fwd-model}, which maps galaxy magnitudes to galaxy colors.
See section \ref{sec:discrete} for data processing bijections, which enable the flow to model discrete data.

Instead of using these data processing bijections, you can of course manually pre-process the data before evaluating densities and post-process samples drawn from the normalizing flow.
However, by building pre- and post-processing directly into the bijection, you remove these extra steps from the workflow.
This reduces the complexity of working with the normalizing flow and ensures that the flow always ``remembers'' how to correctly perform these pre- and post-processing steps.


\subsection{Choosing a latent distribution}
\label{sec:latent}

In principle, with a sufficiently expressive bijection, the choice of latent distribution does not matter as long as it is a distribution in which you can easily sample and calculate densities.
However, in practice, bijections are limited in expressiveness, i.e. they cannot necessarily transform any arbitrary data distribution into any arbitrary latent distribution.

For example, the splines of RQ-RSCs only transform samples in the range [-B, B].
Sampling from a latent distribution with support outside this range will therefore result in strange outliers and incorrect boundary conditions.
One can apply a transformation to the latent samples before they are fed into the RQ-RSC to ensure that they lie within the support of the splines, but it is simpler to use a compact latent distribution whose support matches that of the spline layers.
A simple choice would be the uniform distribution over [-B, B].

Additionally, as no bijector is perfect, the structure of the latent distribution will not be completely erased in the translation from latent to data distribution.
Thus, the latent distribution can be viewed as a prior or inductive bias on samples from the data distribution \citep{jaini2020}.
It is therefore advantageous to select a latent distribution whose features match some of the structure in the data.

A latent distribution that can achieve both desiderata is the Beta distribution, i.e. $u \sim \mathrm{Beta}(\alpha, \beta)$, where $\alpha, \beta > 0$ are learnable parameters\footnote{In practice, it is easier to learn $\log\alpha$ and $\log\beta$ to ensure that $\alpha, \beta > 0$.}.
This distribution is compact, and by varying $\alpha$ and $\beta$ this distribution can take on a wide variety of shapes with different means, skews, and kurtoses, allowing the inductive bias of the prior to adapt to structure in the data during training.
However, as RQ-RSCs are defined on the domain $[-B, B]$, it is more convenient to use a modified Beta distribution, which we name the \emph{Centered Beta distribution}:
\begin{align}
    \text{CentBeta}(u | \alpha, \beta, B) = 2B\left(\text{Beta}(u|\alpha, \beta) - \frac{1}{2}\right).
\end{align}

In general, as long as sampling and density evaluation are tractable, one can use any parameterization of the latent distribution that matches some desired structure in the data and learn the distribution parameters during training.
We give this generalization the name \emph{latent-adaptive flows} (LAFs; inspired by the Tail Adaptive Flows of \citealt{jaini2020}).
Our experiments indicate that learnable latent distributions can improve training loss, but require more care in training.

Note that while we discussed univariate distributions above, these considerations generalize easily to multiple dimensions.
Each of these distributions have multivariate generalizations that can be used when modeling higher-dimensional data.
The full multivariate latent distribution can also be assembled by taking the product of multiple univariate distributions\footnote{Note that while the latent variables will be independent, the data variables will still have correlations imprinted by the bijections.}.
This may even be desired if different dimensions of the data have different structure that you wish to encode in the latent distribution.


\subsection{Conditional flows}
\label{sec:conditional}

The bijectors and latent distributions discussed above can be easily adapted to directly learn conditional probability distributions:
you only need to make the replacement $f(x) \to f(x;y)$, where $y$ is a vector of conditions \citep{winkler2019}.
In practice, as the bijection $f$ is parameterized by a neural network, this can be simply achieved by appending the vector $y$ to the neural network's input.

While $p(x|y)$ is technically encoded within $p(x,y)$, which can be learned with a regular normalizing flow, directly modeling $p(x|y)$ with a conditional flow has a few benefits.
Training is typically faster, since the latent distribution has a smaller number of dimensions.
You can also draw samples of $x$ at fixed values of the conditions $y$, and you can calculate $p(x|y)$ without having to numerically calculate and divide by $p(y)$, which can be computationally expensive.

\subsection{Flows with periodic topology}
\label{sec:periodic}

The flows we have considered so far model data that live in $\R^n$.
This assumption is insufficient for modeling variables from spaces with non-Euclidean topology, e.g. positions on the sky.
While progress has been made on building flows for general topologies (e.g. \citealt{gemici2016} and \citealt{falorsi2019}), we will focus on building flows on the sphere, $S^2$, as this is the case most relevant in astronomy.
We will see that by carefully choosing the latent space, we can construct flows with periodic topology with minimal additional effort \citep{rezende2020}.

Positions on the sphere are specified by two angles\footnote{
We use the physicists' convention where $\theta$ and $\phi$ are the zenith and azimuthal angles, respectively.
},
$\theta$ and $\phi$, the latter of which is periodic.
By mapping $\theta$ to $\cos\theta$, we map the sphere to a cylinder\footnote{
This map can be explicitly constructed via an embedding in $\R^3$.
Technically, the map is not defined for $\theta \in \{0, \pi\}$, however as this set has zero measure, it can be safely ignored.}:
$S^2 \to [-1,1] \times S^1$ (i.e. the Cartesian product of an interval and a circle).
In other words, we can transform $\cos\theta$ with a Euclidean flow, as long as we ensure that the flow bounds samples to the range $[-1, 1]$.
However, the $S^1$ piece, $\phi$, has a periodic topology and must be handled more carefully.

First, we will address transformations of $\cos\theta$.
The only constraint we must impose is that samples of $\cos\theta$ must lie in the range $[-1, 1]$.
Fortunately, RQ-NSCs are bounded, mapping a range in $u$ to the same range in $x$.
Thus, if we pick a latent distribution with compact support in $[-1, 1]$, samples of $\cos\theta$ are guaranteed to lie in the same range, as long as we set the range of the RQ-NSC $B$ = 1.

Next we will address transformations of $\phi$.
For $f$ to be a valid diffeomorphism on the circle, $S^1$, it is sufficient that $f$ obey the following constraints:
\begin{align}
    f(0) &= 0 \\
    f(2\pi) &= 2\pi \\
    \nabla f(0) &= \nabla f(2\pi) \label{eq:df=df} \\
    \nabla f(\phi) &> 0.
\end{align}
The first two constraints ensure continuity of $f$ by designating $\phi=0$ as a fixed point, and the third constraint ensures continuity of $\nabla f$ at that fixed point.
While the designation of $\phi=0$ as a fixed point is an unnecessary restriction on $f$, any diffeomorphism on the circle has at least one fixed point up to a phase change, and so this restriction does not actually restrict the expressiveness of $f$.
The fourth restriction ensures monotonicity, which guarantees invertibility.

If we make the phase change $\phi \to \phi - \pi$ so that our angles $\phi \in [-\pi, \pi]$, a RQ-NSC with $B=\pi$ automatically fulfills all four constraints.
In fact, regular RQ-NSC's impose the further condition
\begin{align}
    \nabla f(-\pi) = \nabla f(\pi) = 1 \label{eq:df=1}
\end{align}
to match an identity transform for inputs outside of the range $[-\pi, \pi]$.
By choosing a latent distribution with compact support in the range $[-\pi, \pi]$, we ensure that no samples will lie outside the range of the splines, and so we can relax the boundary condition of Equation \ref{eq:df=1} in favor of the boundary condition in Equation \ref{eq:df=df}.
Spline transforms with this relaxed boundary condition are named \emph{Circular Splines} by \citet{rezende2020}.

The circular spline construction above is easily generalized to n-spheres and n-tori: $S^n \to [-1, 1]^{n-1} \times S^1$ and $T^n \to (S^1)^n$ (see \citealt{rezende2020} for more details).
We can model the joint distribution of periodic and non-periodic variables with RQ-RSCs simply by choosing appropriate bounds $B$ for each dimension, and by swapping boundary condition \ref{eq:df=1} for condition \ref{eq:df=df} for any periodic dimensions.


\subsection{Modeling discrete variables}
\label{sec:discrete}

In addition to the continuous variables described above, normalizing flows can also be used to model discrete variables.
This can be achieved by ``dequantizing'' the discrete dimensions of the data, which can then be mapped onto continuous latent distributions using regular continuous bijectors.
Dequantization consists of adding some kind of continuous noise to the discrete dimensions, transforming them into continuous dimensions.
When sampling from the flow, you simply do the opposite, and ``quantize'' the discrete dimensions after applying all of the regular bijections, mapping the noisy, continuous variables onto their discrete counterparts.

A common method for dequantization is uniform dequantization, in which random uniform noise in the range (0, 1) is added to the discrete dimensions.
The corresponding quantization applied while sampling from the flow consists of applying the floor function to the dequantized dimensions, mapping these samples onto the nearest integer less than the sampled value.
More sophisticated dequantization schemes use variational inference or even another normalizing flow to determine the noise distributions, which improves results by smoothing the discontinuities between neighboring discrete values.
See \citet{ho2019} \citet{hoogeboom2020} for more details.

While the dequantizers are not technically bijections, they can be treated as data processing bijections and be chained together with the other bijectors in your normalizing flow.


\section{PZFlow}
\label{sec:pzflow}

PZFlow is a Python package for building normalizing flows, with a focus on easy high-performance modeling of high-dimensional tabular data.
Data is handled in Pandas DataFrames \citep{pandas}, while the normalizing flows are implemented in Jax \citep{jax}, which allows for efficient, parallelizable, GPU-enabled calculations for very large data sets.
The code is easily installable for the Python Package Index\footnote{\url{https://pypi.org/project/pzflow/}} (PyPI) and is hosted on Github,\footnote{\url{https://github.com/jfcrenshaw/pzflow}}.
The documentation\footnote{\url{https://jfcrenshaw.github.io/pzflow/}} includes tutorial notebooks demonstrating the features mentioned in this paper on different example problems.

The rest of this paper will demonstrate using PZFlow for the statistical modeling of galaxy catalogs.
Section \ref{sec:galaxy-catalog} uses PZFlow to forward model a photometric catalog, including spectroscopic redshifts, Rubin photometry, ``true'' photo-z posteriors, ellipticities, and sizes.
Section \ref{sec:photo-z} uses PZFlow for photo-z estimation, demonstrating the power of PZFlow as a density estimator, including numerous useful features for photo-z estimation.

In addition to the examples in this paper, PZFlow has already being used in various other projects:
\begin{itemize}
    \item \citet{malz2021} used PZFlow to build a metric for observing strategy optimization based on information theory;
    \item \citet{stylianou2022} used PZFlow to forward model galaxy data with true redshift posteriors in order to evaluate the impact of survey incompleteness and spec-z errors on photo-z estimation;
    \item \citet{lokken2022} used PZFlow to smooth high-redshift artifacts in simulations of host galaxies for supernovae and other transients.
\end{itemize}


\section{Forward Modeling a Galaxy Catalog}
\label{sec:galaxy-catalog}

In this section, we apply PZFlow to forward model a photometric galaxy catalog for the Rubin Observatory.
The advantage of using a catalog generated from a normalizing flow is that we have direct access to the exact distribution from which the data is drawn, enabling us to calculate truth values for derived statistical products, such as the true photo-z redshift posterior for each galaxy.

In Section \ref{sec:fwd-model} we construct a normalizing flow to model the galaxy redshifts and photometry and generate a new simulated catalog.
In Section \ref{sec:true-posteriors}, we calculate true redshift posteriors for the new catalog.
In Section \ref{sec:fwd-model-conditional} we build a conditional flow to add additional properties to the catalog.

\subsection{Forward modeling redshifts and photometry}
\label{sec:fwd-model}

\begin{figure*}[t!]
    \script{forward_model/plot_main_galaxy_corner.py}
    \begin{centering}
        \includegraphics{figures/main_galaxy_corner.pdf}
        \caption{
            Distribution of the CosmoDC2 test set compared to the distribution learned by PZFlow.
            The close overlap of every pair-wise distribution demonstrates that PZFlow was able to learn the structure present in CosmoDC2 with high fidelity.
        }
        \label{fig:main-corner}
    \end{centering}
\end{figure*}

\begin{figure*}[t!]
    \script{forward_model/plot_smooth_color_distribution.py}
    \begin{centering}
        \includegraphics{figures/smooth_color_distribution.pdf}
        \caption{
            Comparing the $r-i$ vs redshift distribution for galaxy samples from CosmoDC2 (left) and from the normalizing flow (right).
            The high-redshift galaxies in CosmoDC2 lie along discrete tracks in color space due to the discrete number of galaxy SED templates used in the simulation.
            PZFlow smooths over these discrete tracks, resulting in a color distribution that is smooth to high redshifts.
        }
        \label{fig:smooth-color-dist}
    \end{centering}
\end{figure*}

To create a generative model of galaxy redshifts and photometry, we use the true redshifts and $ugrizy$ magnitudes from the DESC CosmoDC2 simulation \citep{dc2, cosmodc2}.
We selected all galaxies from CosmoDC2 with at most one band beyond the LSST 10-year 5-sigma point source depths.
Of these, we selected $10^6$ galaxies and split them intro training and test sets consisting of 80\% and 20\% of the galaxies, respectively.

For the latent distribution we use a 7 dimensional Uniform distribution over the range $[-5, 5]$.
To map the data onto the latent distribution, we use the following bijection:
\begin{align}
    f = \text{RQ-RSC} \circ \text{Shift Bounds} \circ \text{Color Transform}.
\end{align}
We will explain each layer of the bijection in the order they are applied to the input data.

The first layer of the bijection is a Color Transform.
This bijection converts galaxy magnitudes to colors, but keeps the $i$ band magnitude as a proxy for the apparent luminosity:
\begin{multline}
    \text{Color Transform} : (\text{redshift},\, u,\, g,\, r,\, i,\, z,\, y) \to \\
    (\text{redshift},\, i,\, u-g,\, g-r,\, r-i,\, i-z,\, z-y).
\end{multline}
This layer is useful as galaxy redshifts correlate more directly with galaxy colors than galaxy magnitudes.

The next layer, Shift Bounds, is the data processing bijection defined in Section \ref{sec:data-processing}, which maps the range of the data onto the support of the RQ-RSC.
Note that since Shift Bounds is on the ``other side'' of the Color Transform, we need to map the ranges of the colors $u-g$, $g-r$, etc. onto the support of the splines, instead of the original magnitudes.

The final layer is an RQ-RSC, described in detail in Section \ref{sec:rq-rsc}.
This layer performs the heavy lifting of transforming the data distribution into the uniform latent distribution.
We use $D=7$ layers to transform all 7 dimensions of our data, and set $B=5$ to match the support of the latent distribution.
We use the coupling function (with two hidden layers of 128 neurons) described in \citet{durkan2019}.
We use $K=16$ spline knots.

After training the flow (see Appendix \ref{app:training-details}), we assess the results by drawing $10^4$ galaxies from the trained flow, and plotting their distribution against  $10^4$ galaxies from the test set (Figure \ref{fig:main-corner}).
We see that the normalizing flow has done an excellent job of reproducing the distribution of DESC DC2, without any unusual artifacts or outliers.
In addition, Figure \ref{fig:smooth-color-dist} compares the distribution of galaxy $r-i$ vs redshift.
The CosmoDC2 simulation is known to exhibit discrete tracks in this space at high redshift, due to the discrete number of SED templates used during simulation.
These tracks are visible in the left panel.
The right panel shows that PZFlow smooths over this discreteness, resulting in a color distribution that is smooth up to high redshifts.

We note that these results were obtained without any extensive hyperparameter search, and that very similar (slightly worse results) are obtained without the \texttt{ColorTransform} bijection, demonstrating the flexibility of the method to adapt to unseen data sets.

With this normalizing flow, we have an efficient CosmoDC2 emulator that produces a smooth distribution of realistic galaxies up to high-redshifts.

To simulate photometric errors, we use the LSST error model of \citet{ivezic2019}, except without the high-SNR assumptions made there (see Appendix \ref{app:error-model} for details).

\subsection{Calculating true posteriors}
\label{sec:true-posteriors}

Since we have direct access to the probability distribution from which the photometry and redshifts are drawn, we can calculate the true redshift posterior for each galaxy: $p(z|m)$ where $m$ is the vector of galaxy magnitudes.
We note that this is not an estimate, like what would be returned by a photo-z estimator, but rather the truth, obtained from the model that generated the photometry and redshifts in the first place.

In addition to calculating the true posterior, $p(z|m)$, we can calculate a true posterior that is consistent with the photometric errors:
\begin{align}
    p(z|m, \sigma_m) = \int p(z|\hat{m}) p(\hat{m}|m, \sigma_m) d\hat{m},
    \label{eq:err-conv}
\end{align}
where $\sigma_m$ is the vector of photometric errors returned by the error model, and $\hat{m}$ are possible noisy observations of the true magnitudes, $m$.
We can evaluate this integral numerically by sampling $\hat{m} \sim p(\hat{m}|m, \sigma_m)$, evaluating $p(z, \hat{m})$ on a redshift grid, and normalizing to obtain $p(z|\hat{m})$.
Averaging $p(z|\hat{m})$ over the samples $\hat{m}$ yields $p(z|m, \sigma_m)$.

We can also marginalize over any missing bands, $n$:
\begin{align}
    p(z|\hat{m}) = \frac{1}{p(\hat{m})} \int p(z, n, \hat{m}) dn,
\end{align}
which can be calculated by evaluating $p(z, n, \hat{m})$ on a grid of $z$ and $n$, summing over $n$ to yield $p(z, \hat{m})$, and normalizing with respect to redshift to yield $p(z|\hat{m})$.
You can once again average over $\hat{m}$ samples to convolve the photometric errors.
You may wish to marginalize over all values of $n$ if the galaxy was not observed in that band.
This might occur, for example, if you include photometry from Euclid, which will not have complete coverage of the LSST catalog \citep{euclid}.
You may also wish to marginalize over all values beyond a limiting magnitude if the galaxy was observed, but not detected in that band.
This might occur, for example, in the low wavelength bands of Lyman dropout galaxies observed by LSST.

\begin{figure}[t!]
    \script{forward_model/plot_posteriors.py}
    \begin{centering}
        \includegraphics{figures/posteriors.pdf}
        \caption{
            True redshift posteriors for a galaxy representing different amounts of information.
            The black posterior is calculated with the true magnitudes;
            the blue posterior is calculated after adding photometric errors;
            the orange posterior is calculated after adding photometric errors, but with the errors convolved during posterior calculation;
            the green posteriors is the same as the orange, except with the $u$ band marginalized over.
        }
        \label{fig:posteriors}
    \end{centering}
\end{figure}

Figure \ref{fig:posteriors} shows a number of redshift posteriors to visualize the effects of error convolution and band marginalization.
The black posterior is calculated using the true galaxy magnitudes, while the blue posteriors is calculated after adding photometric errors.
Calculating the posterior using the noisy photometry results in a posterior with very little support over the true redshift.
The orange posterior has had the photometric errors convolved as in Equation \ref{eq:err-conv}.
Convolving the errors broadens the posterior so that the true redshift lies within the support of the posterior.
This broadening reflects the increased uncertainty due to the photometric errors.
Finally, the green posterior has had the $u$ band marginalized.
This posterior favors lower redshifts, although has a high-redshift tail that covers the support of the previous posteriors. 
This demonstrates how the $u$ band helps constrain the redshift, and the loss of this information leads to greater uncertainty.

Calculating these posteriors enables direct comparison of true redshift posteriors (consistent with the photometric errors and missing bands) with the redshift posteriors estimated by photo-z estimators.
This allows you to test photo-z estimation on a posterior-per-galaxy basis, which will alleviate the photo-z validation problems discovered by \citet{schmidt2020} and will be investigated in future DESC papers.


\subsection{Additional properties with conditional flows}
\label{sec:fwd-model-conditional}

\begin{figure*}[t!]
    \script{forward_model/plot_conditional_galaxy_corner.py}
    \begin{centering}
        \includegraphics{figures/conditional_galaxy_corner.pdf}
        \caption{
            Distribution of the ellipticities and sizes of the galaxies in the CosmoDC2 test set compared to the distribution learned by PZFlow.
            The close overlap of every pair-wise distribution demonstrates that PZFlow was able to learn the structure present in CosmoDC2 with high fidelity.
        }
        \label{fig:conditional-corner}
    \end{centering}
\end{figure*}

In addition to the galaxy magnitudes and redshifts modeled above, we wish to include other galaxy properties in the catalog, such as galaxy size and ellipticity.
In principle, we could have included these variables in the original normalizing flow.
However, we did not want the true redshift posteriors to be conditioned on these variables, as most photo-z estimators only use galaxy photometry.
Therefore, we will build a second flow that models these additional values conditioned on the galaxy redshift and magnitudes.
Note that while we have only chosen to model these additional two properties, any other values you desire can be similarly modeled.

For the latent distribution, we once more use a Uniform distribution over the range $[-5, 5]$.
For the bijection, we use
\begin{align}
    \begin{split}
        f =& ~ \text{RQ-RSC} \circ \text{Shift Bounds}. \\
    \end{split}
\end{align}
The RQ-RSC acts on the two dimensional space of size and ellipticity, but also takes the galaxy redshift and magnitudes as inputs.
The redshifts and magnitudes are transformed to have zero mean and unit variance before being input to the neural network\footnote{These variables are standard scaled instead of mapped onto the domain [-5, 5], because the neural network that parameterizes the splines has no limit on inputs, unlike the splines themselves, which are limited to the range [-5, 5].} that parameterizes the splines.
Aside from the change in inputs, the RQ-RSC has the same settings as listed for the previous normalizing flow.

After training the flow (see Appendix \ref{app:training-details}), we sample a size and ellipticity for each galaxy in the PZFlow catalog created in the previous section (using the true magnitudes), and plot the distribution of these features against the distribution in the test set (Figure \ref{fig:conditional-corner}).
Once again, we see that the normalizing flow does a good job of emulating the CosmoDC2 distribution.

The final simulated catalog consists of $10^4$ galaxies, each with a redshift, $ugrizy$ magnitudes including photometric errors, a true photo-z posterior consistent with the photometric errors, a size, and an ellipticity.
This small catalog was generated for visualization purposes, but the normalizing flows can be used to generate catalogs of arbitrarily large size.


\section{Photometric Redshift Estimation}
\label{sec:photo-z}

\begin{figure*}[t!]
    \script{photo-z/plot_ensemble_posteriors.py}
    \begin{centering}
        \includegraphics{figures/ensemble_posteriors.pdf}
        \caption{
            The ensemble of posteriors for three example galaxies.
            Flows 1-4 label the individual posteriors produced by each of the flows that make up the ensemble.
            The dashed black line is the mean of these individual posteriors and is the value used by the ensemble.
            The vertical gray line labeled ``Truth'' denotes the true redshift of the galaxy.
            Note these galaxies were specifically chosen for their broad, multimodal posteriors.
            The posteriors of most galaxies are unimodal with sharp peaks.
        }
        \label{fig:ensemble-posteriors}
    \end{centering}
\end{figure*}

In addition to forward modeling, normalizing flows are powerful and flexible models for density estimation.
This makes them useful tools for estimating posterior distributions for galaxy properties, conditioned on observed features of the galaxy.
A common example of this in cosmology is photometric redshift estimation, in which you try to estimate the redshift of a galaxy using its magnitude in several photometric bands.
In this section, we will demonstrate this application to show how normalizing flows can be used for density estimation.

\subsection{Training an Ensemble for photo-z estimation}

When forward modeling in Section \ref{sec:galaxy-catalog}, we wanted a realistic model that captured the relevant correlations between galaxy photometry, redshift, shape, and size.
However, when estimating redshifts, we do not simply want a realistic model, but rather a model that matches our galaxy sample as closely as possible.

When training deep learning models, the huge parameter space contains many different solutions, corresponding to different local minima in the parameter space.
In the forward modeling application, we were content with finding a good local minimum, but in this application, we want to marginalize over the different potential models.

A full marginalization over the model parameters would be too computationally expensive, so instead we opt for an ensemble of normalizing flows.
In other words, we will train multiple normalizing flows under identical conditions, just with different random initializations of the model parameters.
This will allow the optimization algorithm to explore different basins of attraction in the parameter space, allowing us to marginalize over different potential models.
In the machine learning literature, this is known as a Deep Ensemble \citep{lakshminarayanan2017}, a popular method for approximate bayesian marginalization \citep{wilson2020,fort2020}.

We train an ensemble of 4 normalizing flows, each with the same architecture and training schedule as the flow described in Section \ref{sec:galaxy-catalog}.
PZFlow makes this easy to do automatically, by simply swapping \texttt{FlowEnsemble} for \texttt{Flow} in the code.

For the training set, we use 100,000 galaxies from the catalog created in Section \ref{sec:galaxy-catalog}.
Each galaxy in the training set has a true redshift and observed magnitudes in the $ugrizy$ bands, with corresponding photometric errors.
To account for the photometric error, at the start of each training epoch, we resample the training set from the photometric error distribution.
In other words, each epoch, we resample
\begin{align}
    m \sim p(\hat{m}, \sigma_m),
\end{align}
where $\hat{m}$ are the observed magnitudes with photometric errors $\sigma_m$, and $p(\hat{m}, \sigma_m)$ is a Gaussian in flux space.
This allows our ensemble of flows to approximate the distribution $p(z, m)$.

The training curves for the flows in the ensemble can be seen in Appendix \ref{app:training-details}.


\subsection{Estimating posteriors}

Each flow in the ensemble estimates the posterior for each galaxy by marginalizing over the photometric errors:
\begin{align}
    p(z| \hat{m}, \sigma_m) \propto \int dm \, p(z, m) \, p(m| \hat{m}, \sigma_m),
\end{align}
which is estimated by sampling $m \sim p(\hat{m}, \sigma_m)$ and averaging $p(z, m)$ over these samples.
We then average the posteriors from each flow in the ensemble.

An example of this for three galaxies can be seen in Figure \ref{fig:ensemble-posteriors}.
Each flow produces a PDF which may contain slightly different features in each case.
By averaging over the individual posteriors, we select for features that are common between models, while smoothing over features that are present in only a single model.
Compare these to the true posteriors.

We can calculate posteriors for galaxies with missing magnitudes by marginalizing over the missing magnitudes.
This is done by averaging $p(z, m)$ over a grid of values for the missing bands. 
This might be useful when not every galaxy is observed in every band, in which case you can marginalize over the bands that were not observed.
Or if a galaxy was observed but not detected in the band, you can marginalize over values for the band that are beyond the limiting magnitude.
You can also marginalize over bands for which you have a magnitude to understand what information about redshift is included in that band.

\begin{figure}[t!]
    \script{photo-z/plot_marginalized_posterior.py}
    \begin{centering}
        \includegraphics{figures/posterior_marginalized.pdf}
        \caption{
            Redshift posteriors for an example galaxy.
            The vertical red line indicates the true redshift.
            The black posterior is estimated using the information from all 6 LSST bands (and includes convolution of the photometric errors).
            The gold posterior has had the $u$ band marginalized, which opens up a second high-redshift mode.
        }
        \label{fig:posterior_marginalized}
    \end{centering}
\end{figure}

Figure \ref{fig:posterior_marginalized} compares posteriors with and without $u$ band information for an example galaxy.
It is clear how losing the $u$ band information increases the uncertainty in the redshift inference.
In this instance, a second potential redshift mode is created.
Conversely, this demonstrates how the addition of $u$ band information rules out the higher redshift solution for this galaxy.


\subsection{Photo-z metrics}

In this section, we calculate common photo-z metrics to evaluate how well the ensemble of flows captures the information present in the catalog.
We calculate each metric for the full catalog and the sub catalog with missing $u$ bands.
Note that these metrics are optimistic in the sense that the training set is representative of the test set, which is usually not the case when applied to real galaxies.

The most common metrics for photo-z estimation concern photo-z point estimates, which are a compression of the photo-z posterior to a single redshift estimate.
We make the common choice of selecting the mode of the posteriors\footnote{The mean redshift is often a poor choice, since photo-z posteriors are generally multimodal, and so the mean value can lie between two modes at a redshift with very small probability density.}.
We compute metrics of the quantity $\Delta z = (z_\text{phot} - z_\text{true}) / (1 + z_\text{true})$, where the denominator accounts for naturally greater uncertainties at high redshift.

Figure \ref{fig:binned-metrics} shows the metrics from \citet{descSRD} as a function of true redshift.
The bias is defined as the median of $\Delta z$; the scatter is defined as $\text{IQR} / 1.349$, where IQR is the interquartile range of $\Delta z$; the outlier fraction is defined as the fraction of galaxies for which $\Delta z$ is greater than three times the $\Delta z$ scatter.
As a point of comparison, we plot the requirements for LSST cosmology in gray.

Like many photo-z estimators, our estimator performs well to a redshift of approximately 1.5.
At higher redshifts, our estimator does not meet the bias and scatter requirements, because there is very little training data in this redshift range.
We note however that for many cosmology application, it is okay for the bias to exceed the required limits, as long as the bias can be well determined via some calibration process \citep{newman2022}.

\begin{figure}[t!]
    \script{photo-z/plot_pz_point_estimates.py}
    \begin{centering}
        \includegraphics{figures/pz_point_estimates.pdf}
        \caption{
            Comparing the photo-z point estimates to the true redshifts in the catalog.
        }
        \label{fig:point-estimates}
    \end{centering}
\end{figure}

Another common metric is the probability integral transform (PIT) (see e.g. \citealt{schmidt2020, dey2022}).
The PIT metric is used to determine if posteriors are well calibrated, i.e. if true values fall within X\% confidence intervals X\% of the time.
The PIT histogram for our estimator is shown in Figure \ref{fig:pit-histogram}.
Ideally, this histogram would be uniform and match the dashed horizontal line.
The fact that the histogram bulges at the center indicates that our estimator is too conservative -- i.e. the posteriors it produces are too broad.
This can be explained by the fact that normalizing flows exhibit mode covering behavior (the opposite of the mode collapse seen in GANs; \citealt{salimans2016}).
In other words, because normalizing flows are trained by maximizing the likelihood of the training samples, they receive very high penalties for missing any modes in the data.
As a result, they tend to conservatively spread out their density, in order to avoid missing any modes.
This results in overly conservative posterior predictions.
The low values at the edges of the PIT histogram indicate the relative rarity of catastrophic outliers, which is also reflected in the far right panel of Figure \ref{fig:binned-metrics}, where you can see that our estimator meets the requirement on the outlier fraction at all redshifts.

\begin{figure*}[t!]
    \script{photo-z/plot_binned_metrics.py}
    \begin{centering}
        \includegraphics{figures/binned_metrics.pdf}
        \caption{
            The bias, scatter, and catastrophic outlier fraction of the photo-z point estimates as a function of true galaxy redshift.
            The dashed black lines represent the science requirements for LSST cosmology as stated in the LSST SRD \citep{descSRD}.
            These lines are to provide a sense of scale for these metrics.
            You can see that PZFlow meets the bias and scatter requirements up to redshift $\sim$ 1.5, while meeting the outlier fraction requirements for all redshifts.
            We note that individual redshifts do not actually need to meet the bias requirement as long as the bias can be well calibrated via some other source, e.g. galaxy clustering.
        }
        \label{fig:binned-metrics}
    \end{centering}
\end{figure*}

The previous metrics analyze photo-z performance for point estimates, which are insufficient for modern cosmology \citep{newman2022}, and for ensembles of posteriors, which is often not a good indicator of photo-z performance for science applications \citep{schmidt2020}.
The methods introduced in this paper allow one to create galaxy catalogs for which each galaxy has a true redshift posterior, which will enable more comprehensive evaluation of photo-z estimators.
A comprehensive evaluation of the full posteriors from photo-z estimators is a goal of our future work.

\begin{figure}[t!]
    \script{photo-z/plot_pit_histogram.py}
    \begin{centering}
        \includegraphics{figures/pit_histogram.pdf}
        \caption{
            The histogram of the probability integral transform (PIT) values of the photo-z posteriors.
            This figure characterizes the calibration of the estimated posteriors.
            In other words, does the true redshift lie at the center/in the tails of the posteriors as often as one would expect?
            Ideally, this histogram should be flat, matching the dashed black line.
            The fact that this distribution has a central peak indicates that the flow ensemble produces posteriors that are a little too conservative in their predictions.
            The slight rightward tilt indicates that the posteriors also have a negative redshift bias.
        }
        \label{fig:pit-histogram}
    \end{centering}
\end{figure}


\section{Conclusion \& Summary}
\label{sec:conclusion}

In this paper we introduced PZFlow, a normalizing flow package in Python, designed for the statistical modeling of astronomical data sets.
We used PZFlow to forward model galaxy catalogs that include photometry, redshifts, sizes, and ellipticities.
In addition, each galaxy in our catalog has a true redshift posterior, which can be convolved with measurement errors.
These true posteriors allow a direct evaluation of the posteriors produced by photo-z estimators.
A similar comparison can be made to posterior estimates for any other galaxy property.

Direct evaluation of photo-z posteriors is vital for future cosmology analyses which must use all of the information incorporated in the full redshift posteriors \citep{newman2022}.
This is because only the full redshift posteriors allow one to account for degeneracies in color-redshift space, which will otherwise bias cosmological inference.
Previous evaluations of photo-z performance focused on point estimates and metrics for ensembles of posteriors, but \citet{schmidt2020} showed these analyses are inadequate for modern cosmology.
This work enables a future analysis of photo-z estimators on a per-posterior basis, which is a major goal of the LSST DESC.

In addition to forward modeling galaxy catalogs, we demonstrated PZFlow's utility as a density estimator that can be applied to photo-z estimation and other statistical analyses.
PZFlow achieves high accuracy with relatively little fine tuning and with very few modeling assumptions.
PZFlow will be a powerful tool for the statistical analysis of tabular astronomical data.

Note that the code to reproduce the figures in this paper is hosted publicly on Github\footnote{\url{https://github.com/jfcrenshaw/pzflow-paper}}, and the code for each individual figure can be reached by clicking on the Github logo in the margin next to the figure.

\begin{acknowledgements}
    JFC is supported by...
\end{acknowledgements}

\software{
    adam \citep{adam},
    corner \citep{corner},
    dill \citep{dill},
    jax \citep{jax},
    jupyter \citep{jupyter},
    matplotlib \citep{matplotlib},
    numpy \citep{numpy},
    pandas \citep{pandas,pandas-software},
    scipy \citep{scipy},
    showyourwork \citep{showyourwork},
    scikit-learn \citep{sklearn}
}

\appendix

\section{Training details}
\label{app:training-details}

\begin{figure*}[t!]
    \script{forward_model/plot_galaxy_losses.py}
    \begin{centering}
        \includegraphics{figures/galaxy_flow_losses.pdf}
        \caption{
            Training losses for the galaxy flows.
            Left: losses for the normal flow.
            After epochs 50 and 100, you can see a drop in the loss due to the decrease in the learning rate.
            Right: losses for the conditional flow.
            After epochs 150 and 300, you can see a drop in the loss due to the decrease in the learning rate.
        }
        \label{fig:galaxy-flow-losses}
    \end{centering}
\end{figure*}

\begin{figure}[t!]
    \script{photo-z/plot_ensemble_losses.py}
    \begin{centering}
        \includegraphics{figures/ensemble_losses.pdf}
        \caption{
            Training losses for the four flows in the flow ensemble.
            We have zoomed in to the bottom of the loss curve so you can see that each of the flows converges to a different minimum loss.
        }
        \label{fig:ensemble-losses}
    \end{centering}
\end{figure}

In this section we list some technical details of training the normalizing flows.
Every flow is trained via minimizing the negative log-likelihood
\begin{align}
    \mathcal{L} = - \, \mathbb{E}[ \, \log p(x) \, ],
\end{align}
where the expectation is performed over galaxies in the training set and $p(x)$ is defined in Equation \ref{eq:px}.

For the main flow in Section \ref{sec:galaxy-catalog}, we trained for 150 epochs.
We used the Adam optimizer \citep{adam}, starting with a learning rate of $10^{-3}$.
We decreased the learning rate by a factor of 10 every 50 epochs.
Training took 7 minutes on a Tesla P100 12GB GPU.
The training loss for this flow is in the left panel of Figure \ref{fig:galaxy-flow-losses}.

For the conditional flow in \ref{sec:galaxy-catalog}, we trained for 450 epochs.
Again, we used Adam with an initial learning rate of $10^{-3}$.
We decreased the learning rate by a factor of 10 every 150 epochs.
The training loss for this flow is in the right panel of Figure \ref{fig:galaxy-flow-losses}.

For each of the flows that make up the flow ensemble in \ref{sec:photo-z}, we trained for 150 epochs using the Adam optimizer.
We started each with a learning rate of $10^{-4}$, and decreased the learning rate by a factor of 10 every 50 epochs.
The training loss for the ensemble is in Figure \ref{fig:ensemble-losses}.
You can see that each of the flows achieves a different minimum loss.
Apparently, each has found a different potential solution in the neural network's parameter space.


\section{LSST Error Model}
\label{app:error-model}

We estimate photometric errors for LSST using a generalization of the error model from \citet{ivezic2019}.
To derive the error model, we start with the noise-to-signal ratio (NSR) for an object with photon count $C$ and background noise $N_0$ (which depends on seeing, read-out noise, etc.):
\begin{align}
    \text{NSR}^2 = \frac{N_0^2 + C}{C^2}.
\end{align}
If we define $C=C_5$ when $\text{NSR}= 1/5$, then we can solve for $N_0$ and write
\begin{align}
    \text{NSR}^2 = \frac{1}{C_5} \left( \frac{C_5}{C} \right) + \left[ \left( \frac{1}{5} \right)^2 - \frac{1}{C_5} \right] \left( \frac{C_5}{C} \right)^2.
\end{align}
Defining $x = C_5/C = 10^{(m-m_5)/2.5}$ and $\gamma = 1/5^2 - 1/C_5$, we have
\begin{align}
    \text{NSR}^2 = (0.04 - \gamma) \, x + \gamma \, x^2 ~~ (\text{mag}^2),
\end{align}
which is Equation 5 from \citet{ivezic2019}.
Values for the band-dependent parameter $\gamma$ can be found in Table 2 of the same paper.

In the high signal-to-noise (SNR) limit, $\text{NSR} \ll 1$, and we can approximate
\begin{align}
    \sigma_\text{rand} = 2.5 \log_{10}(1 + \text{NSR}) \approx \text{NSR}.
    \label{eq:err}
\end{align}
This latter approximation is made by \citet{ivezic2019}, and errors are assumed to be Gaussian in magnitude space.
In contrast, we use the exact form of Equation \ref{eq:err}, and model errors as Gaussian in flux space.
Note that after the photometric errors are applied, the error is re-calculated from the ``observed'' flux, and this new error is reported as the estimated photometric error.
If the original photometric error were reported, it would provide a deterministic link to the original flux.

We have implemented this error model, along with several other extensions to it, in the Python package PhotErr, which is available on the Python Package Index\footnote{\url{https://pypi.org/project/photerr/}} (PyPI), and Github\footnote{\url{https://github.com/jfcrenshaw/photerr}}.
The extensions include different methods for handling non-detections, methods for modeling errors of extended objects \citep{vandenbusch2020,kuijken2019}, and error models for the Roman and Euclid space telescopes \citep{roman,euclid,graham2020}, however, these extensions are not used in this paper.



\bibliography{bib.bib}


\end{document}
